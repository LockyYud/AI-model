{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dO1GLTnM9Mg"
      },
      "source": [
        "# Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEckkRd1M-LI",
        "outputId": "98fbcca6-9b55-4d85-b30f-c192f6d613ae"
      },
      "outputs": [],
      "source": [
        "%pip install -q langchain langchain_community langchain_chroma langchain_experimental langchain-text-splitters\n",
        "%pip install -q langchain-groq langchain_openai\n",
        "%pip install -q langchain-huggingface\n",
        "%pip install -qU langchain-qdrant\n",
        "%pip install -qU qdrant_client\n",
        "%pip install -qU langchain-core\n",
        "%pip install -qU langchain-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIsc0yabNAfU",
        "outputId": "10422163-15b0-41c1-f85d-eb516b0ff7dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "from sklearn.cluster import KMeans\n",
        "from typing import Any\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "from tiktoken import Encoding, encoding_for_model, get_encoding\n",
        "from statistics import mean\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")\n",
        "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
        "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otddDTwpNEIu"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vJuGZNYNFto"
      },
      "outputs": [],
      "source": [
        "#LLMs and embedding model\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "llm_drafter = ChatOpenAI(model=\"gpt-4o-mini\", logprobs=True)\n",
        "llm_verifier = ChatOpenAI(model=\"gpt-4o\", logprobs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYHGFpp1Nmwr"
      },
      "outputs": [],
      "source": [
        "#Vector store\n",
        "qdrant_client = QdrantClient(\n",
        "    url=qdrant_url,\n",
        "    api_key=qdrant_api_key\n",
        ")\n",
        "\n",
        "# Specify your collection name and query vector\n",
        "collection_name = \"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Multi perspective sampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYJF0trOOUBe"
      },
      "outputs": [],
      "source": [
        "def multi_perspective_sampling(\n",
        "    k: int, retrieved_points: list[models.ScoredPoint], seed: int = 1399\n",
        ") -> list[list[str]]:\n",
        "    # Generate clusters\n",
        "    print(f\"Finding {k} clusters.\")\n",
        "    algo: Any = KMeans(n_clusters=k, random_state=seed)\n",
        "    _vectors = [point.vector for point in retrieved_points]\n",
        "    clusters: list[int] = algo.fit_predict(X=_vectors)\n",
        "\n",
        "    # Unique clusters\n",
        "    unique_clusters: set[int] = set(clusters)\n",
        "\n",
        "    # Create a dictionary with the members of each cluster\n",
        "    cluster_dict: defaultdict[int, list[int | None]] = defaultdict(list)\n",
        "    for index, cluster in enumerate(clusters):\n",
        "        cluster_dict[cluster].append(index)\n",
        "    print(f\"Clusters distribution: {dict(cluster_dict)}\")\n",
        "\n",
        "    # M subsets\n",
        "    m: int = min(len(indices) for indices in cluster_dict.values())\n",
        "    print(f\"{m} document subsets will be created.\")\n",
        "\n",
        "    np.random.seed(seed=seed)\n",
        "    subsets: list[list[str]] = []\n",
        "\n",
        "    for _ in range(m):\n",
        "        subset: list[int] = []\n",
        "        for cluster in unique_clusters:\n",
        "            chosen_element: int = np.random.choice(cluster_dict[cluster])\n",
        "            subset.append(chosen_element)\n",
        "            cluster_dict[cluster].remove(chosen_element)\n",
        "        subset_documents = [\n",
        "            retrieved_points[idx].payload.get(\"page_content\") for idx in subset\n",
        "        ]\n",
        "        subsets.append(subset_documents)\n",
        "\n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAG drafting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ1FsMvMRais"
      },
      "outputs": [],
      "source": [
        "#RAG draft\n",
        "prompt_draft = PromptTemplate.from_template(\"\"\"Trả lời câu hỏi dựa trên các văn bản được cung cấp. Đồng thời cung cấp lý do đưa ra câu trả lời của bạn.\n",
        "## Câu hỏi: {query}\n",
        "\n",
        "## Văn bản cung cấp: {evidence}\"\"\")\n",
        "\n",
        "class RagDraftingResponse(BaseModel):\n",
        "    \"\"\"Câu trả lời là lý do vì sao đưa ra câu trả lời như thế\"\"\"\n",
        "    rationale: str = Field(description=\"Lý do đưa ra trả lời.\")\n",
        "    response: str = Field(description=\"Câu trả lời cho câu hỏi.\")\n",
        "llm_drafter_chain = (\n",
        "    prompt_draft\n",
        "    | llm_drafter.with_structured_output(RagDraftingResponse, include_raw=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### RAG verifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e5JC34xbd1P"
      },
      "outputs": [],
      "source": [
        "#RAG verify\n",
        "prompt_verify = PromptTemplate.from_template(\"\"\"## Câu hỏi: {query}\n",
        "\n",
        "## Câu trả lời: {response}\n",
        "\n",
        "## Lý do: {rationale}\n",
        "\n",
        "Lý do có đủ tốt để hỗ trợ cho câu trả lời không? (Có hoặc không)\"\"\")\n",
        "\n",
        "llm_verifier_chain = (\n",
        "    prompt_verify\n",
        "    | llm_verifier\n",
        ")\n",
        "\n",
        "def rag_verifier_generator(subsets, query):\n",
        "    rag_verifications: list[tuple[str, float]] = []\n",
        "    for subset in subsets:\n",
        "        res_draft = llm_drafter_chain.invoke({\"query\": query, \"evidence\": \"\\n\".join([f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)])})\n",
        "        response = llm_verifier_chain.invoke({\"query\": query, \"response\": res_draft.response, \"rationale\": res_draft.rationale})\n",
        "        encoder: Encoding = encoding_for_model(model_name=llm_verifier.model_name)\n",
        "        cond: bool = encoder.encode(text=response.content.lower()) == encoder.encode(text=\"có.\")\n",
        "        p_yes: float = (\n",
        "            np.exp(mean(token['logprob'] for token in response.response_metadata['logprobs']['content']))\n",
        "            if cond\n",
        "            else 0.0\n",
        "        )  # Naive\n",
        "        rag_verifications.append((res_draft.response, p_yes))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### End - to - end"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxZ2u8sjRDoT"
      },
      "outputs": [],
      "source": [
        "seed = 1234\n",
        "query = \"\"\n",
        "query_vector = embeddings.embed_query(query)\n",
        "docs = qdrant_client.search(\n",
        "    collection_name=collection_name,\n",
        "    query_vector=query_vector,\n",
        "    with_vectors=True\n",
        ")\n",
        "subsets = multi_perspective_sampling(k=2, retrieved_points=docs, seed = seed)\n",
        "res_draft = llm_drafter_chain.invoke({\"query\": query, \"evidence\": \"\\n\".join([f\"[{idx}] {doc}\" for idx, doc in enumerate(subsets[1], start=1)])})\n",
        "\n",
        "rag_verifications = rag_verifier_generator(subsets=subsets, query=query)\n",
        "best_answer: int = np.argmax(\n",
        "    draft_score.p_yes for draft_score in rag_verifications\n",
        ")\n",
        "rag_verifications[best_answer][0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
