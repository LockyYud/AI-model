{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dO1GLTnM9Mg"
      },
      "source": [
        "# Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BEckkRd1M-LI",
        "outputId": "98fbcca6-9b55-4d85-b30f-c192f6d613ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m18.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m208.1/208.1 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m599.2/599.2 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.6/94.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.9/399.9 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m292.1/292.1 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.8/273.8 kB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.3/49.3 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.2/13.2 MB\u001b[0m \u001b[31m50.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.5/52.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.7/149.7 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.3/54.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.8/62.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m734.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.7/425.7 kB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m164.1/164.1 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for durationpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.5/51.5 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.5/106.5 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m376.1/376.1 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.3/245.3 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.9/258.9 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.7/5.7 MB\u001b[0m \u001b[31m78.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.6/316.6 kB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires protobuf<5,>=3.20, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-ai-generativelanguage 0.6.6 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-datastore 2.19.0 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "google-cloud-firestore 2.16.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 5.28.2 which is incompatible.\n",
            "opentelemetry-proto 1.27.0 requires protobuf<5.0,>=3.19, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorboard 2.17.0 requires protobuf!=4.24.0,<5.0.0,>=3.19.6, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow 2.17.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3, but you have protobuf 5.28.2 which is incompatible.\n",
            "tensorflow-metadata 1.15.0 requires protobuf<4.21,>=3.20.3; python_version < \"3.11\", but you have protobuf 5.28.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: Could not find a version that satisfies the requirement langchain-hub (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for langchain-hub\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -q langchain langchain_community langchain_chroma langchain_experimental langchain-text-splitters\n",
        "%pip install -q langchain-groq langchain_openai\n",
        "%pip install -q langchain-huggingface\n",
        "%pip install -qU langchain-qdrant\n",
        "%pip install -qU qdrant_client\n",
        "%pip install -qU langchain-core\n",
        "%pip install -qU langchain-hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CIsc0yabNAfU",
        "outputId": "10422163-15b0-41c1-f85d-eb516b0ff7dd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ],
      "source": [
        "import getpass\n",
        "import os\n",
        "import bs4\n",
        "from langchain import hub\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "#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\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = os.getenv(\"LANGCHAIN_TRACING_V2\")\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv(\"LANGCHAIN_API_KEY\")\n",
        "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
        "os.environ[\"GROQ_API_KEY\"] = os.getenv(\"GROQ_API_KEY\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otddDTwpNEIu"
      },
      "source": [
        "# RAG"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-vJuGZNYNFto"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_openai.embeddings import OpenAIEmbeddings\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")\n",
        "llm_drafter = ChatOpenAI(model=\"gpt-4o-mini\", logprobs=True)\n",
        "llm_verifier = ChatOpenAI(model=\"gpt-4o\", logprobs=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wYHGFpp1Nmwr"
      },
      "outputs": [],
      "source": [
        "from qdrant_client import QdrantClient\n",
        "from qdrant_client.http import models\n",
        "qdrant_url = os.getenv(\"QDRANT_URL\")\n",
        "qdrant_api_key = os.getenv(\"QDRANT_API_KEY\")\n",
        "# Create a QdrantClient instance for Qdrant Cloud\n",
        "qdrant_client = QdrantClient(\n",
        "    url=qdrant_url,\n",
        "    api_key=qdrant_api_key\n",
        ")\n",
        "\n",
        "# Specify your collection name and query vector\n",
        "collection_name = \"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xYJF0trOOUBe"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans\n",
        "from typing import Any\n",
        "from collections import defaultdict\n",
        "import numpy as np\n",
        "def multi_perspective_sampling(\n",
        "    k: int, retrieved_points: list[models.ScoredPoint], seed: int = 1399\n",
        ") -> list[list[str]]:\n",
        "    # Generate clusters\n",
        "    print(f\"Finding {k} clusters.\")\n",
        "    algo: Any = KMeans(n_clusters=k, random_state=seed)\n",
        "    _vectors = [point.vector for point in retrieved_points]\n",
        "    clusters: list[int] = algo.fit_predict(X=_vectors)\n",
        "\n",
        "    # Unique clusters\n",
        "    unique_clusters: set[int] = set(clusters)\n",
        "\n",
        "    # Create a dictionary with the members of each cluster\n",
        "    cluster_dict: defaultdict[int, list[int | None]] = defaultdict(list)\n",
        "    for index, cluster in enumerate(clusters):\n",
        "        cluster_dict[cluster].append(index)\n",
        "    print(f\"Clusters distribution: {dict(cluster_dict)}\")\n",
        "\n",
        "    # M subsets\n",
        "    m: int = min(len(indices) for indices in cluster_dict.values())\n",
        "    print(f\"{m} document subsets will be created.\")\n",
        "\n",
        "    # Generate m unique subsets without replacement\n",
        "    np.random.seed(seed=seed)\n",
        "    subsets: list[list[str]] = []\n",
        "\n",
        "    for _ in range(m):\n",
        "        subset: list[int] = []\n",
        "        for cluster in unique_clusters:\n",
        "            chosen_element: int = np.random.choice(cluster_dict[cluster])\n",
        "            subset.append(chosen_element)\n",
        "            cluster_dict[cluster].remove(chosen_element)\n",
        "        subset_documents = [\n",
        "            retrieved_points[idx].payload.get(\"page_content\") for idx in subset\n",
        "        ]\n",
        "        subsets.append(subset_documents)\n",
        "\n",
        "    return subsets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxZ2u8sjRDoT"
      },
      "outputs": [],
      "source": [
        "query = \"\"\n",
        "query_vector = embeddings.embed_query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g6FYRC48QKtf"
      },
      "outputs": [],
      "source": [
        "docs = qdrant_client.search(\n",
        "    collection_name=collection_name,\n",
        "    query_vector=query_vector,\n",
        "    with_vectors=True\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i1NbiHKZQd4r",
        "outputId": "d00ea1be-a571-40c5-d094-016e7ee190d2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Finding 2 clusters.\n",
            "Clusters distribution: {1: [0, 1, 2, 3, 4, 7], 0: [5, 6, 8, 9]}\n",
            "4 document subsets will be created.\n"
          ]
        }
      ],
      "source": [
        "subsets = multi_perspective_sampling(k=2, retrieved_points=docs, seed = 123)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dZ1FsMvMRais"
      },
      "outputs": [],
      "source": [
        "from langchain_core.pydantic_v1 import BaseModel, Field\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_draft = PromptTemplate.from_template(\"\"\"Trả lời câu hỏi dựa trên các văn bản được cung cấp. Đồng thời cung cấp lý do đưa ra câu trả lời của bạn.\n",
        "## Câu hỏi: {query}\n",
        "\n",
        "## Văn bản cung cấp: {evidence}\"\"\")\n",
        "\n",
        "class RagDraftingResponse(BaseModel):\n",
        "    \"\"\"Câu trả lời là lý do vì sao đưa ra câu trả lời như thế\"\"\"\n",
        "    rationale: str = Field(description=\"Lý do đưa ra trả lời.\")\n",
        "    response: str = Field(description=\"Câu trả lời cho câu hỏi.\")\n",
        "llm_drafter_chain = (\n",
        "    prompt_draft\n",
        "    | llm_drafter.with_structured_output(RagDraftingResponse, include_raw=True)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0e5JC34xbd1P"
      },
      "outputs": [],
      "source": [
        "prompt_verify = PromptTemplate.from_template(\"\"\"## Câu hỏi: {query}\n",
        "\n",
        "## Câu trả lời: {response}\n",
        "\n",
        "## Lý do: {rationale}\n",
        "\n",
        "Lý do có đủ tốt để hỗ trợ cho câu trả lời không? (Có hoặc không)\"\"\")\n",
        "\n",
        "llm_verifier_chain = (\n",
        "    prompt_verify\n",
        "    | llm_verifier\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6HqBXnLvmpB"
      },
      "outputs": [],
      "source": [
        "res_draft = llm_drafter_chain.invoke({\"query\": query, \"evidence\": \"\\n\".join([f\"[{idx}] {doc}\" for idx, doc in enumerate(subsets[1], start=1)])})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "teKsSDOgwJVd",
        "outputId": "07947947-6d41-4ebe-dd90-7c37fdd9c46a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'raw': AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_RVHGZjd2kWnyq56Q7bDSErdc', 'function': {'arguments': '{\"rationale\":\"Cách mạng tư sản Pháp nổ ra trong bối cảnh kinh tế, xã hội khó khăn, với tình trạng áp bức nông dân và sự lạc hậu trong sản xuất nông nghiệp. Sự bất mãn của quần chúng nhân dân, đặc biệt là nông dân bị đối xử tàn nhẫn và sự phát triển của nền kinh tế hàng hóa đã tạo ra tiền đề cho cách mạng.\",\"response\":\"Cách mạng tư sản Pháp nổ ra trong hoàn cảnh kinh tế khó khăn, xã hội bất công, nông dân bị áp bức và sự phát triển của nền kinh tế hàng hóa.\"}', 'name': 'RagDraftingResponse'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 135, 'prompt_tokens': 385, 'total_tokens': 520, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_e9627b5346', 'finish_reason': 'stop', 'logprobs': {'content': None, 'refusal': None}}, id='run-ff37655a-e930-421c-9a4d-32ff5c1318ff-0', tool_calls=[{'name': 'RagDraftingResponse', 'args': {'rationale': 'Cách mạng tư sản Pháp nổ ra trong bối cảnh kinh tế, xã hội khó khăn, với tình trạng áp bức nông dân và sự lạc hậu trong sản xuất nông nghiệp. Sự bất mãn của quần chúng nhân dân, đặc biệt là nông dân bị đối xử tàn nhẫn và sự phát triển của nền kinh tế hàng hóa đã tạo ra tiền đề cho cách mạng.', 'response': 'Cách mạng tư sản Pháp nổ ra trong hoàn cảnh kinh tế khó khăn, xã hội bất công, nông dân bị áp bức và sự phát triển của nền kinh tế hàng hóa.'}, 'id': 'call_RVHGZjd2kWnyq56Q7bDSErdc', 'type': 'tool_call'}], usage_metadata={'input_tokens': 385, 'output_tokens': 135, 'total_tokens': 520}),\n",
              " 'parsed': RagDraftingResponse(rationale='Cách mạng tư sản Pháp nổ ra trong bối cảnh kinh tế, xã hội khó khăn, với tình trạng áp bức nông dân và sự lạc hậu trong sản xuất nông nghiệp. Sự bất mãn của quần chúng nhân dân, đặc biệt là nông dân bị đối xử tàn nhẫn và sự phát triển của nền kinh tế hàng hóa đã tạo ra tiền đề cho cách mạng.', response='Cách mạng tư sản Pháp nổ ra trong hoàn cảnh kinh tế khó khăn, xã hội bất công, nông dân bị áp bức và sự phát triển của nền kinh tế hàng hóa.'),\n",
              " 'parsing_error': None}"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "res_draft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jBV6uZWgsvWc"
      },
      "outputs": [],
      "source": [
        "from tiktoken import Encoding, encoding_for_model, get_encoding\n",
        "from statistics import mean\n",
        "\n",
        "rag_verifications: list[tuple[str, float]] = []\n",
        "for subset in subsets:\n",
        "    res_draft = llm_drafter_chain.invoke({\"query\": query, \"evidence\": \"\\n\".join([f\"[{idx}] {doc}\" for idx, doc in enumerate(subset, start=1)])})\n",
        "    response = llm_verifier_chain.invoke({\"query\": query, \"response\": res_draft.response, \"rationale\": res_draft.rationale})\n",
        "    encoder: Encoding = encoding_for_model(model_name=llm_verifier.model_name)\n",
        "    cond: bool = encoder.encode(text=response.content.lower()) == encoder.encode(text=\"có.\")\n",
        "    p_yes: float = (\n",
        "        np.exp(mean(token['logprob'] for token in response.response_metadata['logprobs']['content']))\n",
        "        if cond\n",
        "        else 0.0\n",
        "    )  # Naive\n",
        "    rag_verifications.append((res_draft.response, p_yes))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDXdJw1Ka_K5"
      },
      "outputs": [],
      "source": [
        "best_answer: int = np.argmax(\n",
        "    draft_score.p_yes for draft_score in rag_verifications\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "tQDE1AQbcZxH",
        "outputId": "1bd22cc6-66b6-4e28-af3d-22b9b652678b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Cách mạng tư sản Pháp nổ ra trong hoàn cảnh có sự phân chia giai cấp sâu sắc giữa giai cấp tư sản và giai cấp vô sản, với những điều kiện làm việc và sinh hoạt cực kỳ tồi tệ cho công nhân. Sự phát triển của chủ nghĩa tư bản và khủng hoảng kinh tế đã làm gia tăng mâu thuẫn xã hội, tạo ra bối cảnh thuận lợi cho sự nổ ra của cách mạng.'"
            ]
          },
          "execution_count": 147,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rag_verifications[best_answer][0]"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
